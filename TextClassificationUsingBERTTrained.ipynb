{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ShashankHebbar006/Natural-Language-Processing/blob/main/TextClassificationUsingBERTTrained.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "id": "58ea9f3b-b2af-45aa-a495-6ed549b052b8",
      "metadata": {
        "id": "58ea9f3b-b2af-45aa-a495-6ed549b052b8"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "from torch import nn\n",
        "from transformers import AutoModel, AutoTokenizer, AdamW, get_linear_schedule_with_warmup\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "id": "97ac016c-a0d3-4a4a-afb8-0b6029d30c0c",
      "metadata": {
        "id": "97ac016c-a0d3-4a4a-afb8-0b6029d30c0c"
      },
      "outputs": [],
      "source": [
        "path =\"/content/US-Economic-News.csv\"\n",
        "\n",
        "def load_data(path):\n",
        "    csv_data = pd.read_csv(path,encoding='ISO-8859-1')\n",
        "    csv_data = csv_data[csv_data['relevance'] != 'not sure']\n",
        "    csv_data['relevance'] = csv_data['relevance'].map({'yes':1,'no':0})\n",
        "\n",
        "    text = csv_data['text'].to_list()\n",
        "    labels = csv_data['relevance'].to_list()\n",
        "    return text,labels\n",
        "\n",
        "text, labels = load_data(path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "id": "5f2cc314",
      "metadata": {
        "id": "5f2cc314"
      },
      "outputs": [],
      "source": [
        "class TextClassificationDataset(Dataset):\n",
        "    def __init__(self,text,labels,tokenizer,max_length):\n",
        "        self.text = text\n",
        "        self.labels = labels\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.text)\n",
        "\n",
        "    def __getitem__(self,idx):\n",
        "        text = self.text[idx]\n",
        "        label = self.labels[idx]\n",
        "        encoding = tokenizer(text,return_tensors='pt',max_length=self.max_length,padding='max_length',truncation=True)\n",
        "        return {'input_ids': encoding['input_ids'].flatten(),\n",
        "                'attention_mask':encoding['attention_mask'].flatten(),\n",
        "                'label':torch.tensor(label)}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "id": "31668966",
      "metadata": {
        "id": "31668966"
      },
      "outputs": [],
      "source": [
        "class BERTClassifier(nn.Module):\n",
        "    def __init__(self,checkpoint,num_classes):\n",
        "        super(BERTClassifier,self).__init__()\n",
        "        self.bert = AutoModel.from_pretrained(checkpoint)\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "        self.linear = nn.Linear(self.bert.config.hidden_size,num_classes)\n",
        "\n",
        "    def forward(self,input_ids,attention_mask):\n",
        "        output = self.bert(input_ids,attention_mask)\n",
        "        pooled_output = output.pooler_output\n",
        "        X = self.dropout(pooled_output)\n",
        "        logits = self.linear(X)\n",
        "        return logits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "id": "6e888485",
      "metadata": {
        "id": "6e888485"
      },
      "outputs": [],
      "source": [
        "def train(model,data_loader,optimizer,scheduler,device):\n",
        "    model.train()\n",
        "    for batch in data_loader:\n",
        "        optimizer.zero_grad()\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        labels = batch['label'].to(device)\n",
        "        outputs = model(input_ids,attention_mask)\n",
        "        loss = nn.CrossEntropyLoss()(outputs,labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        scheduler.step()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "id": "f6269aa2",
      "metadata": {
        "id": "f6269aa2"
      },
      "outputs": [],
      "source": [
        "def evaluate(model,data_loader,device):\n",
        "    model.eval()\n",
        "    predictions = []\n",
        "    actuals = []\n",
        "    with torch.no_grad():\n",
        "        for batch in data_loader:\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            labels = batch['label'].to(device)\n",
        "            outputs = model(input_ids,attention_mask)\n",
        "            _, preds = torch.max(outputs,dim=1)\n",
        "            predictions.extend(preds.cpu())\n",
        "            actuals.extend(labels.cpu())\n",
        "    return accuracy_score(actuals,predictions), classification_report(actuals,predictions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "id": "79943a11",
      "metadata": {
        "id": "79943a11"
      },
      "outputs": [],
      "source": [
        "def predict_relevance(text,model,tokenizer,device,max_length=128):\n",
        "    model.eval()\n",
        "    encoding = tokenizer(text,max_length=max_length,return_tensors='pt',padding=True,truncation=True)\n",
        "    input_ids = encoding['input_ids'].to(device)\n",
        "    attention_mask = encoding['attention_mask'].to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(input_ids,attention_mask)\n",
        "        _, preds = torch.max(outputs,dim=1)\n",
        "    return 'YES' if preds.item() == 1 else 'NO'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "id": "e0af451e",
      "metadata": {
        "id": "e0af451e"
      },
      "outputs": [],
      "source": [
        "# Parameters\n",
        "checkpoint ='bert-base-uncased'\n",
        "num_classes = 2\n",
        "max_length = 128\n",
        "batch_size = 16\n",
        "epochs = 4\n",
        "lr = 2e-5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "id": "e882b4a3",
      "metadata": {
        "id": "e882b4a3"
      },
      "outputs": [],
      "source": [
        "train_text, tmp_text, train_labels, tmp_labels = train_test_split(text,labels,train_size=0.8)\n",
        "val_text, test_text, val_labels, test_labels = train_test_split(tmp_text, tmp_labels, train_size=0.5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "id": "460639e4",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "460639e4",
        "outputId": "4a27c2ec-d4ce-4c5b-a5ac-12d1ef35b32e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:72: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
        "\n",
        "tokenized_train_ds = TextClassificationDataset(train_text,train_labels,tokenizer,max_length)\n",
        "tokenized_val_ds = TextClassificationDataset(val_text,val_labels,tokenizer,max_length)\n",
        "tokenized_test_ds = TextClassificationDataset(test_text,test_labels,tokenizer,max_length)\n",
        "\n",
        "tokenized_train_dl = DataLoader(tokenized_train_ds,batch_size,shuffle=True)\n",
        "tokenized_val_dl = DataLoader(tokenized_val_ds,batch_size,shuffle=True)\n",
        "tokenized_test_dl = DataLoader(tokenized_test_ds,batch_size,shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "id": "32ee4637",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "32ee4637",
        "outputId": "31735db9-71c0-4114-cc89-6b47875cbe83"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = BERTClassifier(checkpoint, num_classes).to(device)\n",
        "optimizer = AdamW(model.parameters(), lr=lr)\n",
        "total_steps = len(tokenized_train_dl) * epochs\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "id": "5965c572",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5965c572",
        "outputId": "a00a910b-8f57-41af-8386-0e5a15be59f4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/4\n",
            "Validation Accuracy: 0.8198\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      1.00      0.90       653\n",
            "           1       0.62      0.03      0.06       146\n",
            "\n",
            "    accuracy                           0.82       799\n",
            "   macro avg       0.72      0.51      0.48       799\n",
            "weighted avg       0.79      0.82      0.75       799\n",
            "\n",
            "Epoch 2/4\n",
            "Validation Accuracy: 0.8010\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.86      0.90      0.88       653\n",
            "           1       0.44      0.35      0.39       146\n",
            "\n",
            "    accuracy                           0.80       799\n",
            "   macro avg       0.65      0.63      0.64       799\n",
            "weighted avg       0.78      0.80      0.79       799\n",
            "\n",
            "Epoch 3/4\n",
            "Validation Accuracy: 0.7885\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.87      0.87      0.87       653\n",
            "           1       0.42      0.43      0.43       146\n",
            "\n",
            "    accuracy                           0.79       799\n",
            "   macro avg       0.65      0.65      0.65       799\n",
            "weighted avg       0.79      0.79      0.79       799\n",
            "\n",
            "Epoch 4/4\n",
            "Validation Accuracy: 0.7897\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.87      0.88      0.87       653\n",
            "           1       0.42      0.40      0.41       146\n",
            "\n",
            "    accuracy                           0.79       799\n",
            "   macro avg       0.64      0.64      0.64       799\n",
            "weighted avg       0.79      0.79      0.79       799\n",
            "\n"
          ]
        }
      ],
      "source": [
        "for epoch in range(epochs):\n",
        "    print(f\"Epoch {epoch + 1}/{epochs}\")\n",
        "    train(model, tokenized_train_dl, optimizer, scheduler, device)\n",
        "    accuracy, report = evaluate(model, tokenized_val_dl, device)\n",
        "    print(f\"Validation Accuracy: {accuracy:.4f}\")\n",
        "    print(report)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "id": "998c67b6",
      "metadata": {
        "id": "998c67b6"
      },
      "outputs": [],
      "source": [
        "torch.save(model.state_dict(),'bert_classifier.pth')"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}